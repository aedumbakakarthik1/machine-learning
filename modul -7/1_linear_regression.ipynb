{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYSSpwTLtsOH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "\n",
        "Linear regression is a statistical approach to modeling the relationship between a dependent variable and one or more independent variables. It assumes that the relationship between the variables can be modeled using a linear function. The equation for a linear regression model with a single independent variable is:\n",
        "\n",
        "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* y is the dependent variable\n",
        "* x is the independent variable\n",
        "* $\\beta_0$ is the intercept\n",
        "* $\\beta_1$ is the slope coefficient\n",
        "* $\\epsilon$ is the error term or residual\n",
        "\n",
        "\n",
        "# Linear Regression with One Independent Variable\n",
        "Linear regression is a statistical method for modeling the relationship between a dependent variable and one or more independent variables. When there is only one independent variable, the relationship can be modeled using a simple linear regression equation:\n",
        "\n",
        "$$y = mx + c$$\n",
        "\n",
        "In this equation:\n",
        "\n",
        "* y is the dependent variable\n",
        "* x is the independent variable\n",
        "* m is the slope of the line, representing the change in y per unit change in x\n",
        "* c is the intercept of the line, representing the y-value when x is 0.\n",
        "\n",
        "\n",
        "The goal of linear regression is to estimate the values of the slope and intercept parameters that best fit the data. This is typically done by minimizing the sum of squared errors between the predicted values of y and the actual values of y in the data. The optimization algorithm used to minimize this objective function is often gradient descent, as mentioned in my previous answer.\n",
        "\n",
        "\n",
        "It's worth noting that the notation used for the slope and intercept parameters may vary depending on the field or context in which the regression is being performed. Some fields use β0 and β1 to represent the intercept and slope parameters, respectively, while others use m and c. The important thing is to understand the meaning of the parameters and their role in the regression model.\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n",
        "\n",
        "$$y = wx + b$$\n",
        "\n",
        "* where $y$ is the dependent variable,\n",
        "* $x$ is the independent variable, \n",
        "* $w$ is the slope parameter (also known as the weight or coefficient), and \n",
        "* $b$ is the intercept parameter (also known as the bias).\n",
        "\n",
        "The goal of linear regression is to find the values of $w$ and $b$ that best fit the data. This is typically done by minimizing the mean squared error (MSE) between the predicted values of $y$ and the actual values of $y$ in the training data set. The MSE is defined as:\n",
        "\n",
        "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
        "\n",
        "where $n$ is the number of data points, $y_i$ is the actual value of the dependent variable for the $i$th data point, and $\\hat{y}_i$ is the predicted value of the dependent variable for the $i$th data point, which is calculated as:\n",
        "\n",
        "$$\\hat{y}_i = wx_i + b$$\n",
        "\n",
        "To minimize the MSE, we can use gradient descent to update the values of $w$ and $b$ iteratively. The update rule for $w$ is:\n",
        "\n",
        "$$w = w - \\alpha \\frac{1}{n} \\sum_{i=1}^{n} (wx_i + b - y_i) x_i$$\n",
        "\n",
        "and the update rule for $b$ is:\n",
        "\n",
        "$$b = b - \\alpha \\frac{1}{n} \\sum_{i=1}^{n} (wx_i + b - y_i)$$\n",
        "\n",
        "where $\\alpha$ is the learning rate, which controls the step size of the updates. By iteratively updating $w$ and $b$ using these update rules, we can eventually find the values that minimize the MSE and provide a good fit to the data."
      ],
      "metadata": {
        "id": "KyUF36d7t8rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent\n",
        "Gradient descent is an optimization algorithm that is commonly used to find the minimum of a function. In the context of linear regression, the goal of gradient descent is to find the values of $\\beta_0$ and $\\beta_1$ that minimize the sum of squared errors. The algorithm works by iteratively adjusting the values of $\\beta_0$ and $\\beta_1$ in the direction of the negative gradient of the sum of squared errors.\n",
        "\n",
        "The update rules for gradient descent are:\n",
        "\n",
        "$$\\beta_0^{(t+1)} = \\beta_0^{(t)} - \\alpha \\frac{\\partial}{\\partial \\beta_0} SSE(\\beta_0^{(t)}, \\beta_1^{(t)})$$\n",
        "\n",
        "$$\\beta_1^{(t+1)} = \\beta_1^{(t)} - \\alpha \\frac{\\partial}{\\partial \\beta_1} SSE(\\beta_0^{(t)}, \\beta_1^{(t)})$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $t$ is the current iteration\n",
        "* $\\alpha$ is the learning rate\n",
        "* $SSE$ is the sum of squared errors\n",
        "\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Gradient Descent\n",
        "Gradient descent is a first-order optimization algorithm that is commonly used to find the optimal values of the parameters in a linear regression model. The goal of gradient descent is to minimize the sum of squared errors (SSE) between the predicted values of y and the actual values of y in the data. In simple linear regression with one independent variable, the SSE can be written as:\n",
        "\n",
        "$$SSE = \\sum_{i=1}^{n}(y_i - mx_i - c)^2$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $n$ is the number of observations in the data\n",
        "* $y_i$ is the actual value of the dependent variable for the i-th observation\n",
        "* $x_i$ is the actual value of the independent variable for the i-th observation\n",
        "* $m$ is the slope of the line, representing the change in y per unit change in x\n",
        "* $c$ is the intercept of the line, representing the y-value when x is 0.\n",
        "* Gradient descent works by iteratively updating the values of the parameters in the direction of the negative gradient of the SSE. The update rules for the slope and intercept parameters are:\n",
        "\n",
        "$$m = m - \\alpha \\frac{\\partial}{\\partial m} SSE$$\n",
        "\n",
        "$$c = c - \\alpha \\frac{\\partial}{\\partial c} SSE$$\n",
        "\n",
        "where $\\alpha$ is the learning rate, which determines the step size of the update.\n",
        "\n",
        "\n",
        "-----------------------------------------------------------------\n",
        "for w\n",
        "\n",
        "$$w = w - \\alpha \\frac{\\partial}{\\partial w} J(w)$$\n",
        "\n",
        "where $w$ is the slope parameter, $\\alpha$ is the learning rate, and $J(w)$ is the loss function, which in the case of linear regression with one independent variable is the mean squared error (MSE):\n",
        "\n",
        "$$J(w) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - wx_i)^2$$\n",
        "\n",
        "Taking the partial derivative of the loss function with respect to the slope parameter, we get:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial w} J(w) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - wx_i)(-x_i) = -\\frac{1}{n} \\sum_{i=1}^{n} x_i (y_i - wx_i)$$\n",
        "\n",
        "Substituting this into the update rule, we get:\n",
        "\n",
        "$$w = w - \\alpha \\left(-\\frac{1}{n} \\sum_{i=1}^{n} x_i (y_i - wx_i)\\right) = w + \\frac{\\alpha}{n} \\sum_{i=1}^{n} x_i (y_i - wx_i)$$\n",
        "\n",
        "This is the update rule for the slope parameter in gradient descent for linear regression with one independent variable. The update rule for the intercept parameter is similar, but with the partial derivative of the loss function with respect to the intercept parameter instead.\n",
        "\n",
        "\n",
        "for b\n",
        "\n",
        "\n",
        "$$b = b - \\alpha \\frac{\\partial}{\\partial b} J(w,b)$$\n",
        "\n",
        "where $b$ is the intercept parameter, $\\alpha$ is the learning rate, and $J(w,b)$ is the loss function. In the case of linear regression with one independent variable, the loss function is the mean squared error (MSE):\n",
        "\n",
        "$$J(w,b) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - wx_i - b)^2$$\n",
        "\n",
        "Taking the partial derivative of the loss function with respect to the intercept parameter, we get:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial b} J(w,b) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - wx_i - b)(-1) = -\\frac{1}{n} \\sum_{i=1}^{n}(y_i - wx_i)$$\n",
        "\n",
        "Substituting this into the update rule, we get:\n",
        "\n",
        "$$b = b - \\alpha \\left(-\\frac{1}{n} \\sum_{i=1}^{n}(y_i - wx_i)\\right) = b + \\frac{\\alpha}{n} \\sum_{i=1}^{n}(y_i - wx_i)$$\n",
        "\n",
        "This is the update rule for the intercept parameter in gradient descent for linear regression with one independent variable. The update rule for the slope parameter is as mentioned before, with the partial derivative of the loss function with respect to the slope parameter."
      ],
      "metadata": {
        "id": "Ep9cvpqIvTdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate\n",
        "The learning rate is a hyperparameter that controls the step size in gradient descent. A high learning rate can cause the algorithm to overshoot the minimum, while a low learning rate can cause the algorithm to take a long time to converge. The optimal learning rate depends on the specific problem and data.\n",
        "\n",
        "In general, a good approach is to start with a small learning rate, such as 0.01 or 0.001, and gradually increase it until the algorithm converges to the minimum of the SSE. It's important to monitor the convergence of the algorithm during training to ensure that the learning rate is appropriate.\n",
        "\n",
        "In summary, gradient descent is an optimization algorithm used to find the optimal values of the parameters in a linear regression model. The learning rate is a hyperparameter that controls the step size in gradient descent, and it's important to choose an appropriate value to ensure convergence to the minimum of the SSE.\n",
        "\n",
        "the partial derivative of the mean squared error (MSE) loss function with respect to the weight parameter $w$ in linear regression with one independent variable. This partial derivative is used in gradient descent to update the weight parameter.\n",
        "\n",
        "The full update rule for the weight parameter in gradient descent for linear regression with one independent variable is:\n",
        "\n",
        "$$w = w - \\alpha \\frac{\\partial}{\\partial w} J(w, b)$$\n",
        "\n",
        "where $\\alpha$ is the learning rate, and $J(w, b)$ is the loss function. In the case of linear regression with one independent variable, the loss function is the mean squared error (MSE):\n",
        "\n",
        "$$J(w,b) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - wx_i - b)^2$$\n",
        "\n",
        "Taking the partial derivative of the loss function with respect to the weight parameter $w$, we get:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial w} J(w,b) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - wx_i - b)(-x_i)$$\n",
        "\n",
        "Simplifying this expression, we get:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial w} J(w,b) = -\\frac{1}{n} \\sum_{i=1}^{n}x_i(y_i - wx_i - b)$$\n",
        "\n",
        "Substituting this expression into the update rule for the weight parameter, we get:\n",
        "\n",
        "$$w = w - \\alpha \\left(-\\frac{1}{n} \\sum_{i=1}^{n}x_i(y_i - wx_i - b)\\right) = w + \\frac{\\alpha}{n} \\sum_{i=1}^{n}x_i(y_i - wx_i - b)$$\n",
        "\n",
        "This is the full update rule for the weight parameter in gradient descent for linear regression with one independent variable. The expression you provided is the partial derivative of the loss function with respect to the weight parameter, which is used in this update rule.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "the partial derivative of the mean squared error (MSE) loss function with respect to the bias parameter $b$ in linear regression with one independent variable. This partial derivative is used in gradient descent to update the bias parameter.\n",
        "\n",
        "The full update rule for the bias parameter in gradient descent for linear regression with one independent variable is:\n",
        "\n",
        "$$b = b - \\alpha \\frac{\\partial}{\\partial b} J(w, b)$$\n",
        "\n",
        "where $\\alpha$ is the learning rate, and $J(w, b)$ is the loss function. In the case of linear regression with one independent variable, the loss function is the mean squared error (MSE):\n",
        "\n",
        "$$J(w,b) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_i - wx_i - b)^2$$\n",
        "\n",
        "Taking the partial derivative of the loss function with respect to the bias parameter $b$, we get:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial b} J(w,b) = \\frac{1}{n} \\sum_{i=1}^{n}(y_i - wx_i - b)(-1)$$\n",
        "\n",
        "Simplifying this expression, we get:\n",
        "\n",
        "$$\\frac{\\partial}{\\partial b} J(w,b) = -\\frac{1}{n} \\sum_{i=1}^{n}(y_i - wx_i - b)$$\n",
        "\n",
        "Substituting this expression into the update rule for the bias parameter, we get:\n",
        "\n",
        "$$b = b - \\alpha \\left(-\\frac{1}{n} \\sum_{i=1}^{n}(y_i - wx_i - b)\\right) = b + \\frac{\\alpha}{n} \\sum_{i=1}^{n}(y_i - wx_i - b)$$\n",
        "\n",
        "This is the full update rule for the bias parameter in gradient descent for linear regression with one independent variable. The expression you provided is the partial derivative of the loss function with respect to the bias parameter, which is used in this update rule."
      ],
      "metadata": {
        "id": "WeKw3LqlyMHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the libs\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LYUgrFTAvOG_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**linear regression**"
      ],
      "metadata": {
        "id": "rUHEWULD1a3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class linear_regression :\n",
        "\n",
        "# initiating the parmeters (learning rate & no.of iterations)\n",
        "  def __init__(self,learning_rate,no_of_iterations):\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "  def fit(self,X,Y):\n",
        "    # number of training example & no_of_iterations\n",
        "\n",
        "    self.m,self.n = X.shape  # number of rows & columns\n",
        "\n",
        "    # number of weigth and bias\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0 \n",
        "\n",
        "    self.X=X\n",
        "    self.Y=Y\n",
        "\n",
        "    # implementing Gradient Descent\n",
        "\n",
        "    for  i in range(self.no_of_iterations):\n",
        "      self.update_weigths()\n",
        "  \n",
        "  def update_weigths(self):\n",
        "    Y_prediction = self.predict(self.X)\n",
        "\n",
        "\n",
        "    #calculate the gradient\n",
        "\n",
        "    dw= -(2* (self.X.T).dot(self.Y - Y_prediction)) /self.m\n",
        "\n",
        "    db = -2* np.sum(self.Y - Y_prediction)/self.m\n",
        "\n",
        "\n",
        "    #updating the weigths\n",
        "    self.w = self.w - self.learning_rate * dw\n",
        "    self.b = self.b - self.learning_rate * db\n",
        "\n",
        "  def predict(self,X ):\n",
        "    return X.dot(self.w) + self.b\n"
      ],
      "metadata": {
        "id": "CanZL3ciz0Mz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using linear regression model for pediction"
      ],
      "metadata": {
        "id": "MuV6aXVLGyg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the depeantencies\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "bPvjpdlb2Nex"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data pre-processing"
      ],
      "metadata": {
        "id": "KN8xYiK_36MB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lording the data from csv file a pandas dataframe\n",
        "\n",
        "df=pd.read_csv(\"/content/Salary_Data.csv\")"
      ],
      "metadata": {
        "id": "miZHI-GzAJfP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the first 5 dataset\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "M2y0BoihIpMW",
        "outputId": "0fe75e79-5dac-47e1-a594-a4895982891a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   YearsExperience   Salary\n",
              "0              1.1  39343.0\n",
              "1              1.3  46205.0\n",
              "2              1.5  37731.0\n",
              "3              2.0  43525.0\n",
              "4              2.2  39891.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3dc0e0e9-46a8-4256-bdfe-ebbe01b541a5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YearsExperience</th>\n",
              "      <th>Salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.1</td>\n",
              "      <td>39343.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.3</td>\n",
              "      <td>46205.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.5</td>\n",
              "      <td>37731.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>43525.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.2</td>\n",
              "      <td>39891.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dc0e0e9-46a8-4256-bdfe-ebbe01b541a5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dc0e0e9-46a8-4256-bdfe-ebbe01b541a5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dc0e0e9-46a8-4256-bdfe-ebbe01b541a5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#printing last 5 data sets\n",
        "df.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dYN91G4L4PBE",
        "outputId": "087f45e8-4ba3-4b47-ab0a-a59c777a0f6a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    YearsExperience    Salary\n",
              "25              9.0  105582.0\n",
              "26              9.5  116969.0\n",
              "27              9.6  112635.0\n",
              "28             10.3  122391.0\n",
              "29             10.5  121872.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6e6051ce-e5e1-4343-b202-5feebe58d874\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YearsExperience</th>\n",
              "      <th>Salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>9.0</td>\n",
              "      <td>105582.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>9.5</td>\n",
              "      <td>116969.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>9.6</td>\n",
              "      <td>112635.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>10.3</td>\n",
              "      <td>122391.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>10.5</td>\n",
              "      <td>121872.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e6051ce-e5e1-4343-b202-5feebe58d874')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6e6051ce-e5e1-4343-b202-5feebe58d874 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6e6051ce-e5e1-4343-b202-5feebe58d874');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print the number of rows and columes in dataframe\n",
        "\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvX5q0so4d8Z",
        "outputId": "3c5491ca-a26b-4a69-f15c-724817efedb4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking any missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF5LtdWf4vaa",
        "outputId": "1536a41c-476a-4d16-fdf2-7c2786016514"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YearsExperience    0\n",
              "Salary             0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "or \n",
        "X=df[\"YearsExperience\"]\n",
        "Y=df[\"Salary\"]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "X= df.iloc[:,:-1].values\n",
        "\n",
        "Y= df.iloc[:,1].values\n"
      ],
      "metadata": {
        "id": "soxz128347cy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zot17Hp5RmS",
        "outputId": "54db5a40-19a8-4400-fef3-cf706a2e9e4d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.1],\n",
              "       [ 1.3],\n",
              "       [ 1.5],\n",
              "       [ 2. ],\n",
              "       [ 2.2],\n",
              "       [ 2.9],\n",
              "       [ 3. ],\n",
              "       [ 3.2],\n",
              "       [ 3.2],\n",
              "       [ 3.7],\n",
              "       [ 3.9],\n",
              "       [ 4. ],\n",
              "       [ 4. ],\n",
              "       [ 4.1],\n",
              "       [ 4.5],\n",
              "       [ 4.9],\n",
              "       [ 5.1],\n",
              "       [ 5.3],\n",
              "       [ 5.9],\n",
              "       [ 6. ],\n",
              "       [ 6.8],\n",
              "       [ 7.1],\n",
              "       [ 7.9],\n",
              "       [ 8.2],\n",
              "       [ 8.7],\n",
              "       [ 9. ],\n",
              "       [ 9.5],\n",
              "       [ 9.6],\n",
              "       [10.3],\n",
              "       [10.5]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XC_M57G50Yl",
        "outputId": "59fe21de-c2dd-4c67-d33c-4c8ce85ae71a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 39343.,  46205.,  37731.,  43525.,  39891.,  56642.,  60150.,\n",
              "        54445.,  64445.,  57189.,  63218.,  55794.,  56957.,  57081.,\n",
              "        61111.,  67938.,  66029.,  83088.,  81363.,  93940.,  91738.,\n",
              "        98273., 101302., 113812., 109431., 105582., 116969., 112635.,\n",
              "       122391., 121872.])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spliting the dataset into training and test data"
      ],
      "metadata": {
        "id": "rn0Tqgh26R-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y, test_size=0.33 , random_state= 2)"
      ],
      "metadata": {
        "id": "smjw72aM57oG"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training the linear Regrssion model"
      ],
      "metadata": {
        "id": "N7rz2gMt664i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = linear_regression(learning_rate=0.002,no_of_iterations=1000)"
      ],
      "metadata": {
        "id": "HYbycJX_6y8z"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape,Y_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUMjVtjE8XQN",
        "outputId": "a6e53349-d132-4eff-a439-64cbccd44e8e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 1) (20,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "Aqyvuw247ndF"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the parmeter values (weigth and bias)\n",
        "print(\"weights : \",model.w[0])\n",
        "print(\"bias : \",model.b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMGeP0Rb8Mj5",
        "outputId": "fa42dc7a-f1b7-4e4e-9fce-3b5cc8f86c14"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights :  10945.020784583106\n",
            "bias :  13472.151907043792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict the salary values for test data"
      ],
      "metadata": {
        "id": "SQloTtGt-179"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_prediction = model.predict(X_test)\n",
        "test_data_prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RiVRXjn-HgX",
        "outputId": "faa85daf-87ef-44c5-85a7-b89e318705cf"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 27700.678927  ,  25511.67477009,  62724.74543767,  53968.72881   ,\n",
              "        91181.79947758,  79142.27661454, 103221.32234063,  46307.21426079,\n",
              "        35362.19347621,  87898.29324221])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualizling the predicted values & actual value"
      ],
      "metadata": {
        "id": "edZQyhu2_NgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_test,Y_test,color=\"red\")\n",
        "plt.plot(X_test,test_data_prediction,color=\"blue\")\n",
        "\n",
        "plt.xlabel(\"Years of Experience\")\n",
        "plt.ylabel(\"salary\")\n",
        "plt.title(\"salary vs experience\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ORqxPkSY-r1t",
        "outputId": "d209c616-764e-48cf-d1b3-4e365c502f84"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "hJRZOXwR_lSt"
      },
      "execution_count": 72,
      "outputs": []
    }
  ]
}